<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>genra.rax.skl.cls API documentation</title>
<meta name="description" content="GenRA Classifier â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>genra.rax.skl.cls</code></h1>
</header>
<section id="section-intro">
<p>GenRA Classifier</p>
<p>Adapted from sklearn.neighbors.KNeighborsClassifier</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
GenRA Classifier

Adapted from sklearn.neighbors.KNeighborsClassifier

&#34;&#34;&#34;

# Authors: Imran Shah (shah.imran@epa.gov)

import numpy as np
import sklearn
from sklearn.base import ClassifierMixin
from sklearn.neighbors._base import BaseEstimator, NeighborsBase,\
        KNeighborsMixin, SupervisedIntegerMixin,\
        _check_weights, _get_weights
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted,_is_arraylike, _num_samples
from sklearn.utils.multiclass import unique_labels
from scipy import stats
from sklearn.utils.extmath import weighted_mode

import warnings

class GenRAPredClass(NeighborsBase, KNeighborsMixin,
                     SupervisedIntegerMixin, ClassifierMixin):
    &#34;&#34;&#34;GenRA Classifier implementing the k-nearest neighbors vote.

    Parameters
    ----------
    n_neighbors : int, optional (default = 5)
        Number of neighbors to use by default for :meth:`kneighbors` queries.

    algorithm : {&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;, &#39;brute&#39;}, optional
        Algorithm used to compute the nearest neighbors:

        - &#39;ball_tree&#39; will use :class:`BallTree`
        - &#39;kd_tree&#39; will use :class:`KDTree`
        - &#39;brute&#39; will use a brute-force search.
        - &#39;auto&#39; will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.

    leaf_size : int, optional (default = 30)
        Leaf size passed to BallTree or KDTree.  This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree.  The optimal value depends on the
        nature of the problem.

    metric : string or callable, default &#39;1-jaccard&#39;
        the distance metric to use for the tree.  The default metric is
        1-jaccard, and with p=2 is equivalent to the standard Euclidean
        metric. See the documentation of the DistanceMetric class for a
        list of available metrics.

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    n_jobs : int
    or None, optional (default=None)
        The number of parallel jobs to run for neighbors search.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
        for more details.
        Doesn&#39;t affect :meth:`fit` method.

    Attributes
    ----------
    classes_ : array of shape (n_classes,)
        Class labels known to the classifier

    effective_metric_ : string or callble
        The distance metric used. It will be same as the `metric` parameter
        or a synonym of it, e.g. &#39;euclidean&#39; if the `metric` parameter set to
        &#39;minkowski&#39; and `p` parameter set to 2.

    effective_metric_params_ : dict
        Additional keyword arguments for the metric function. For most metrics
        will be same with `metric_params` parameter, but may also contain the
        `p` parameter value if the `effective_metric_` attribute is set to
        &#39;minkowski&#39;.

    outputs_2d_ : bool
        False when `y`&#39;s shape is (n_samples, ) or (n_samples, 1) during fit
        otherwise True.

    &#34;&#34;&#34;

    def __init__(self, n_neighbors=5,
                 weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, leaf_size=30,
                 p=2, metric=&#39;jaccard&#39;, metric_params=None, n_jobs=None,
                 **kwargs):
        super().__init__(
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size, metric=metric, p=p,
            metric_params=metric_params,
            n_jobs=n_jobs, **kwargs)
        self.weights = _check_weights(weights)

    def predict(self, X):
        &#34;&#34;&#34;Predict the class labels for the provided data.

        Parameters
        ----------
        X : array-like, shape (n_queries, n_features), \
                or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
            Test samples.

        Returns
        -------
        y : array of shape [n_queries] or [n_queries, n_outputs]
            Class labels for each data sample.
        &#34;&#34;&#34;
        X = check_array(X, accept_sparse=&#39;csr&#39;)

        neigh_dist, neigh_ind = self.kneighbors(X)

        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]

        n_outputs = len(classes_)
        n_queries = _num_samples(X)
        weights = _get_weights(neigh_dist, self.weights)

        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)
        for k, classes_k in enumerate(classes_):
            if weights is None:
                mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
            else:
                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)

            mode = np.asarray(mode.ravel(), dtype=np.intp)
            y_pred[:, k] = classes_k.take(mode)

        if not self.outputs_2d_:
            y_pred = y_pred.ravel()

        return y_pred
        
    
    def predict_proba(self, X):
        &#34;&#34;&#34;Return probability estimates for the test data X.

        Parameters
        ----------
        X : array-like, shape (n_queries, n_features), \
                or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
            Test samples.

        Returns
        -------
        p : array of shape = [n_queries, n_classes], or a list of n_outputs
            of such arrays if n_outputs &gt; 1.
            The class probabilities of the input samples. Classes are ordered
            by lexicographic order.
        &#34;&#34;&#34;
        X = check_array(X, accept_sparse=&#39;csr&#39;)

        neigh_dist, neigh_ind = self.kneighbors(X)

        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]

        n_queries = _num_samples(X)

        weights = _get_weights(neigh_dist, self.weights)
        if weights is None:
            weights = np.ones_like(neigh_ind)

        all_rows = np.arange(X.shape[0])
        probabilities = []
        for k, classes_k in enumerate(classes_):
            pred_labels = _y[:, k][neigh_ind]
            proba_k = np.zeros((n_queries, classes_k.size))

            # a simple &#39;:&#39; index doesn&#39;t work right
            for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)
                proba_k[all_rows, idx] += weights[:, i]

            # normalize &#39;votes&#39; into real [0,1] probabilities
            normalizer = proba_k.sum(axis=1)[:, np.newaxis]
            normalizer[normalizer == 0.0] = 1.0
            proba_k /= normalizer

            probabilities.append(proba_k)

        if not self.outputs_2d_:
            probabilities = probabilities[0]

        return probabilities</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="genra.rax.skl.cls.GenRAPredClass"><code class="flex name class">
<span>class <span class="ident">GenRAPredClass</span></span>
<span>(</span><span>n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='jaccard', metric_params=None, n_jobs=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>GenRA Classifier implementing the k-nearest neighbors vote.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n_neighbors</code></strong> :&ensp;<code>int</code>, optional <code>(default = 5)</code></dt>
<dd>Number of neighbors to use by default for :meth:<code>kneighbors</code> queries.</dd>
<dt><strong><code>algorithm</code></strong> :&ensp;<code>{'auto', 'ball_tree', 'kd_tree', 'brute'}</code>, optional</dt>
<dd>
<p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li>'ball_tree' will use :class:<code>BallTree</code></li>
<li>'kd_tree' will use :class:<code>KDTree</code></li>
<li>'brute' will use a brute-force search.</li>
<li>'auto' will attempt to decide the most appropriate algorithm
based on the values passed to :meth:<code>fit</code> method.</li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</dd>
<dt><strong><code>leaf_size</code></strong> :&ensp;<code>int</code>, optional <code>(default = 30)</code></dt>
<dd>Leaf size passed to BallTree or KDTree.
This can affect the
speed of the construction and query, as well as the memory
required to store the tree.
The optimal value depends on the
nature of the problem.</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>string</code> or <code>callable</code>, default <code>'1-jaccard'</code></dt>
<dd>the distance metric to use for the tree.
The default metric is
1-jaccard, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of the DistanceMetric class for a
list of available metrics.</dd>
<dt><strong><code>metric_params</code></strong> :&ensp;<code>dict</code>, optional <code>(default = None)</code></dt>
<dd>Additional keyword arguments for the metric function.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>or None, optional (default=None)
The number of parallel jobs to run for neighbors search.
<code>None</code> means 1 unless in a :obj:<code>joblib.parallel_backend</code> context.
<code>-1</code> means using all processors. See :term:<code>Glossary &lt;n_jobs&gt;</code>
for more details.
Doesn't affect :meth:<code>fit</code> method.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>classes_</code></strong> :&ensp;<code>array</code> of <code>shape (n_classes,)</code></dt>
<dd>Class labels known to the classifier</dd>
<dt><strong><code>effective_metric_</code></strong> :&ensp;<code>string</code> or <code>callble</code></dt>
<dd>The distance metric used. It will be same as the <code>metric</code> parameter
or a synonym of it, e.g. 'euclidean' if the <code>metric</code> parameter set to
'minkowski' and <code>p</code> parameter set to 2.</dd>
<dt><strong><code>effective_metric_params_</code></strong> :&ensp;<code>dict</code></dt>
<dd>Additional keyword arguments for the metric function. For most metrics
will be same with <code>metric_params</code> parameter, but may also contain the
<code>p</code> parameter value if the <code>effective_metric_</code> attribute is set to
'minkowski'.</dd>
<dt><strong><code>outputs_2d_</code></strong> :&ensp;<code>bool</code></dt>
<dd>False when <code>y</code>'s shape is (n_samples, ) or (n_samples, 1) during fit
otherwise True.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GenRAPredClass(NeighborsBase, KNeighborsMixin,
                     SupervisedIntegerMixin, ClassifierMixin):
    &#34;&#34;&#34;GenRA Classifier implementing the k-nearest neighbors vote.

    Parameters
    ----------
    n_neighbors : int, optional (default = 5)
        Number of neighbors to use by default for :meth:`kneighbors` queries.

    algorithm : {&#39;auto&#39;, &#39;ball_tree&#39;, &#39;kd_tree&#39;, &#39;brute&#39;}, optional
        Algorithm used to compute the nearest neighbors:

        - &#39;ball_tree&#39; will use :class:`BallTree`
        - &#39;kd_tree&#39; will use :class:`KDTree`
        - &#39;brute&#39; will use a brute-force search.
        - &#39;auto&#39; will attempt to decide the most appropriate algorithm
          based on the values passed to :meth:`fit` method.

        Note: fitting on sparse input will override the setting of
        this parameter, using brute force.

    leaf_size : int, optional (default = 30)
        Leaf size passed to BallTree or KDTree.  This can affect the
        speed of the construction and query, as well as the memory
        required to store the tree.  The optimal value depends on the
        nature of the problem.

    metric : string or callable, default &#39;1-jaccard&#39;
        the distance metric to use for the tree.  The default metric is
        1-jaccard, and with p=2 is equivalent to the standard Euclidean
        metric. See the documentation of the DistanceMetric class for a
        list of available metrics.

    metric_params : dict, optional (default = None)
        Additional keyword arguments for the metric function.

    n_jobs : int
    or None, optional (default=None)
        The number of parallel jobs to run for neighbors search.
        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`
        for more details.
        Doesn&#39;t affect :meth:`fit` method.

    Attributes
    ----------
    classes_ : array of shape (n_classes,)
        Class labels known to the classifier

    effective_metric_ : string or callble
        The distance metric used. It will be same as the `metric` parameter
        or a synonym of it, e.g. &#39;euclidean&#39; if the `metric` parameter set to
        &#39;minkowski&#39; and `p` parameter set to 2.

    effective_metric_params_ : dict
        Additional keyword arguments for the metric function. For most metrics
        will be same with `metric_params` parameter, but may also contain the
        `p` parameter value if the `effective_metric_` attribute is set to
        &#39;minkowski&#39;.

    outputs_2d_ : bool
        False when `y`&#39;s shape is (n_samples, ) or (n_samples, 1) during fit
        otherwise True.

    &#34;&#34;&#34;

    def __init__(self, n_neighbors=5,
                 weights=&#39;uniform&#39;, algorithm=&#39;auto&#39;, leaf_size=30,
                 p=2, metric=&#39;jaccard&#39;, metric_params=None, n_jobs=None,
                 **kwargs):
        super().__init__(
            n_neighbors=n_neighbors,
            algorithm=algorithm,
            leaf_size=leaf_size, metric=metric, p=p,
            metric_params=metric_params,
            n_jobs=n_jobs, **kwargs)
        self.weights = _check_weights(weights)

    def predict(self, X):
        &#34;&#34;&#34;Predict the class labels for the provided data.

        Parameters
        ----------
        X : array-like, shape (n_queries, n_features), \
                or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
            Test samples.

        Returns
        -------
        y : array of shape [n_queries] or [n_queries, n_outputs]
            Class labels for each data sample.
        &#34;&#34;&#34;
        X = check_array(X, accept_sparse=&#39;csr&#39;)

        neigh_dist, neigh_ind = self.kneighbors(X)

        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]

        n_outputs = len(classes_)
        n_queries = _num_samples(X)
        weights = _get_weights(neigh_dist, self.weights)

        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)
        for k, classes_k in enumerate(classes_):
            if weights is None:
                mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
            else:
                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)

            mode = np.asarray(mode.ravel(), dtype=np.intp)
            y_pred[:, k] = classes_k.take(mode)

        if not self.outputs_2d_:
            y_pred = y_pred.ravel()

        return y_pred
        
    
    def predict_proba(self, X):
        &#34;&#34;&#34;Return probability estimates for the test data X.

        Parameters
        ----------
        X : array-like, shape (n_queries, n_features), \
                or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
            Test samples.

        Returns
        -------
        p : array of shape = [n_queries, n_classes], or a list of n_outputs
            of such arrays if n_outputs &gt; 1.
            The class probabilities of the input samples. Classes are ordered
            by lexicographic order.
        &#34;&#34;&#34;
        X = check_array(X, accept_sparse=&#39;csr&#39;)

        neigh_dist, neigh_ind = self.kneighbors(X)

        classes_ = self.classes_
        _y = self._y
        if not self.outputs_2d_:
            _y = self._y.reshape((-1, 1))
            classes_ = [self.classes_]

        n_queries = _num_samples(X)

        weights = _get_weights(neigh_dist, self.weights)
        if weights is None:
            weights = np.ones_like(neigh_ind)

        all_rows = np.arange(X.shape[0])
        probabilities = []
        for k, classes_k in enumerate(classes_):
            pred_labels = _y[:, k][neigh_ind]
            proba_k = np.zeros((n_queries, classes_k.size))

            # a simple &#39;:&#39; index doesn&#39;t work right
            for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)
                proba_k[all_rows, idx] += weights[:, i]

            # normalize &#39;votes&#39; into real [0,1] probabilities
            normalizer = proba_k.sum(axis=1)[:, np.newaxis]
            normalizer[normalizer == 0.0] = 1.0
            proba_k /= normalizer

            probabilities.append(proba_k)

        if not self.outputs_2d_:
            probabilities = probabilities[0]

        return probabilities</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.neighbors._base.NeighborsBase</li>
<li>sklearn.base.MultiOutputMixin</li>
<li>sklearn.base.BaseEstimator</li>
<li>sklearn.neighbors._base.KNeighborsMixin</li>
<li>sklearn.neighbors._base.SupervisedIntegerMixin</li>
<li>sklearn.base.ClassifierMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="genra.rax.skl.cls.GenRAPredClass.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the class labels for the provided data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_queries, n_features),</code>
or <code>(n_queries, n_indexed) if metric == 'precomputed'</code></dt>
<dd>Test samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>array</code> of <code>shape [n_queries]</code> or <code>[n_queries, n_outputs]</code></dt>
<dd>Class labels for each data sample.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;Predict the class labels for the provided data.

    Parameters
    ----------
    X : array-like, shape (n_queries, n_features), \
            or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
        Test samples.

    Returns
    -------
    y : array of shape [n_queries] or [n_queries, n_outputs]
        Class labels for each data sample.
    &#34;&#34;&#34;
    X = check_array(X, accept_sparse=&#39;csr&#39;)

    neigh_dist, neigh_ind = self.kneighbors(X)

    classes_ = self.classes_
    _y = self._y
    if not self.outputs_2d_:
        _y = self._y.reshape((-1, 1))
        classes_ = [self.classes_]

    n_outputs = len(classes_)
    n_queries = _num_samples(X)
    weights = _get_weights(neigh_dist, self.weights)

    y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)
    for k, classes_k in enumerate(classes_):
        if weights is None:
            mode, _ = stats.mode(_y[neigh_ind, k], axis=1)
        else:
            mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)

        mode = np.asarray(mode.ravel(), dtype=np.intp)
        y_pred[:, k] = classes_k.take(mode)

    if not self.outputs_2d_:
        y_pred = y_pred.ravel()

    return y_pred</code></pre>
</details>
</dd>
<dt id="genra.rax.skl.cls.GenRAPredClass.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Return probability estimates for the test data X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like, shape (n_queries, n_features),</code>
or <code>(n_queries, n_indexed) if metric == 'precomputed'</code></dt>
<dd>Test samples.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>p</code></strong> :&ensp;<code>array</code> of <code>shape = [n_queries, n_classes],</code> or <code>a list</code> of <code>n_outputs</code></dt>
<dd>of such arrays if n_outputs &gt; 1.
The class probabilities of the input samples. Classes are ordered
by lexicographic order.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    &#34;&#34;&#34;Return probability estimates for the test data X.

    Parameters
    ----------
    X : array-like, shape (n_queries, n_features), \
            or (n_queries, n_indexed) if metric == &#39;precomputed&#39;
        Test samples.

    Returns
    -------
    p : array of shape = [n_queries, n_classes], or a list of n_outputs
        of such arrays if n_outputs &gt; 1.
        The class probabilities of the input samples. Classes are ordered
        by lexicographic order.
    &#34;&#34;&#34;
    X = check_array(X, accept_sparse=&#39;csr&#39;)

    neigh_dist, neigh_ind = self.kneighbors(X)

    classes_ = self.classes_
    _y = self._y
    if not self.outputs_2d_:
        _y = self._y.reshape((-1, 1))
        classes_ = [self.classes_]

    n_queries = _num_samples(X)

    weights = _get_weights(neigh_dist, self.weights)
    if weights is None:
        weights = np.ones_like(neigh_ind)

    all_rows = np.arange(X.shape[0])
    probabilities = []
    for k, classes_k in enumerate(classes_):
        pred_labels = _y[:, k][neigh_ind]
        proba_k = np.zeros((n_queries, classes_k.size))

        # a simple &#39;:&#39; index doesn&#39;t work right
        for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)
            proba_k[all_rows, idx] += weights[:, i]

        # normalize &#39;votes&#39; into real [0,1] probabilities
        normalizer = proba_k.sum(axis=1)[:, np.newaxis]
        normalizer[normalizer == 0.0] = 1.0
        proba_k /= normalizer

        probabilities.append(proba_k)

    if not self.outputs_2d_:
        probabilities = probabilities[0]

    return probabilities</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="genra.rax.skl" href="index.html">genra.rax.skl</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="genra.rax.skl.cls.GenRAPredClass" href="#genra.rax.skl.cls.GenRAPredClass">GenRAPredClass</a></code></h4>
<ul class="">
<li><code><a title="genra.rax.skl.cls.GenRAPredClass.predict" href="#genra.rax.skl.cls.GenRAPredClass.predict">predict</a></code></li>
<li><code><a title="genra.rax.skl.cls.GenRAPredClass.predict_proba" href="#genra.rax.skl.cls.GenRAPredClass.predict_proba">predict_proba</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>